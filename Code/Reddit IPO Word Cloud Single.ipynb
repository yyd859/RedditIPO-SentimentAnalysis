{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ee84b6-6413-4330-90ad-fd00d889dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kevin\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee4c2f-f679-4ec4-9637-03b5927a2c91",
   "metadata": {},
   "source": [
    "# Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6688c4-ad5b-4b10-8b6c-3c3b75ccd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9d96d6-b227-4fea-a080-8f7d69836f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"C:/Users/kevin/OneDrive/Winter Case Competition/RedditIPO-SentimentAnalysis/Data\"\n",
    "os.chdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e06b1a6-fa33-411a-b12f-cd19653d82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading text\n",
    "posts = pd.read_csv('posts_rd.csv')\n",
    "comments = pd.read_csv('comments_rd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5cfc738-022f-457f-87ed-870e7d70edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.ogrid[:600, :600]\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152802b-02b7-4433-9aaa-14f139d9bd8c",
   "metadata": {},
   "source": [
    "# Trim dataframe to list & dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f371e7b3-de15-47e6-9bfa-40235df1b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_trimm = posts[['title', 'body']]\n",
    "comments_trimm = comments[['comment_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db800718-bfc7-48fe-8a99-f9ccbb1bfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts\n",
    "posts_trimm = posts_trimm.to_string()\n",
    "comments_trimm = comments_trimm.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c26052c-a7b2-4964-9eae-293ff36666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonkenizer. Lemmatizer is used here to achieve full morphological analysis and accurately identify the lemma for each word, this is better than simply stemming. \n",
    "WNL = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d7275-b490-4652-a524-8b997f03b40b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc694c45-a1ed-47b9-aebb-fa557e657860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up texts by transforming to lower case, also removing apostrophe\n",
    "def lowerRep(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text\n",
    "\n",
    "# removing number\n",
    "def removeNum(text):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    return text\n",
    "\n",
    "# remove extra chars and stop words\n",
    "def removeChar(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text1 = nltk.Text(tokens)\n",
    "    text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text1]\n",
    "    return text_content\n",
    "\n",
    "# remove URLs and images\n",
    "def removeURL(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# remove ![img] string\n",
    "def removeImg(text):\n",
    "    text = re.sub(r'!\\[img]\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# initial removal of stopwords and empty spaces\n",
    "def removeSpace(text, stopwords):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    text = [s for s in text if len(s) != 0]\n",
    "    return text\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(text):\n",
    "    text = [WNL.lemmatize(t) for t in text]\n",
    "    return text\n",
    "\n",
    "# tokenize\n",
    "def tokenize(text):\n",
    "    tokens = [nltk.word_tokenize(i) for i in text]\n",
    "    return tokens\n",
    "\n",
    "# sigle word dictionary\n",
    "def singleDict(text):\n",
    "    dictionary = [' '.join(tup) for tup in text]\n",
    "    return dictionary\n",
    "\n",
    "# generate words frequency\n",
    "def wordsFreq(text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag_of_words = vectorizer.fit_transform(text)\n",
    "    vectorizer.vocabulary_\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c2442-d0eb-498e-8eed-d8ed7d20c750",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "564b22bc-87b7-4032-8bbf-fcbadd6f6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "posts_stopwords = list(STOPWORDS) + [\"https\", \"png\", \"imgur\", \"n\", \"click\", \"reddit\", \"ashx\", \"will\", \"CHART\", \n",
    "                                     \"t\", \"ta\", \"st_c\", \"sma\", \"smsch_200p\", \"bb_20_2\", \"webp\", \"s\", \"l\", \n",
    "                                    \"stofu_b_14_3_3\", \"macd_b_12_26_9\", \"rsi_b_14\", \"sch_200p\", \"p\", \"d\", \"c\", \"nn\"]\n",
    "comments_stopwords = list(STOPWORDS) + [\"n\", \"nbsp\", \"http\", \"u\", \"s\", \"reddit\", \"will\", \"tth\", \"emote\", \"nn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f388a2-87c8-4db6-b5c9-c98a44faf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# string1 = \"![img](emote|t5_2th52|4258)\"\n",
    "# string2 = \"https://github.com/yyd859/RedditIPO-SentimentAnalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3af9ef-5050-4ad8-a8cc-69e7ca3432ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeImg(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ed97da3-d10e-4e4a-ad2c-f6662d86cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeURL(string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62a37d-6898-4331-aead-6260580d444a",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72338282-e765-4f81-9c95-b46e57d8a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for posts\n",
    "posts_trimm = removeURL(posts_trimm)\n",
    "posts_trimm = removeImg(posts_trimm)\n",
    "posts_trimm = lowerRep(posts_trimm)\n",
    "posts_trimm = removeNum(posts_trimm)\n",
    "posts_content = removeChar(posts_trimm)\n",
    "posts_content = removeSpace(posts_content, posts_stopwords)\n",
    "posts_content = lemmatize(posts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee5ec200-ebc9-48c1-a49f-55d83f6fb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for comments: this should take longer\n",
    "comments_trimm = removeURL(comments_trimm)\n",
    "comments_trimm = removeImg(comments_trimm)\n",
    "comments_trimm = lowerRep(comments_trimm)\n",
    "comments_trimm = removeNum(comments_trimm)\n",
    "comments_content = removeChar(comments_trimm)\n",
    "comments_content = removeSpace(comments_content, comments_stopwords)\n",
    "comments_content = lemmatize(comments_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343c74f-607d-4980-b5dd-f42a6e7f24ba",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c25bb2-a816-41d6-b76c-87348b04e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate dictionary for posts\n",
    "posts_token = tokenize(posts_content)\n",
    "# text2 = nltk.Text(posts_token)\n",
    "posts_single = singleDict(nltk.Text(posts_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20f260b-1b23-4f38-956a-7d0b2458fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate dictionary for comments\n",
    "comments_token = tokenize(comments_content)\n",
    "comments_single = singleDict(nltk.Text(comments_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9714f92e-24fe-4d57-bf43-02432ae82558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "# print(posts_token)\n",
    "# print(posts_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f0f0393-5fa7-485f-b967-1658edb8a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for posts\n",
    "posts_word_freq = wordsFreq(posts_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c41bfb68-e406-4838-a446-46bb659fd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "# print(posts_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16b133e8-247c-4608-9053-af8cf36c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for comments\n",
    "comments_word_freq = wordsFreq(comments_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eeaa137-c913-4dd3-88fa-00d4e3972bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_dict = dict(posts_word_freq)\n",
    "comments_dict = dict(comments_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74313112-508d-49a2-a2cb-98317c7e3382",
   "metadata": {},
   "source": [
    "# Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca914e16-3e74-48b7-a5fb-a5eb2f881785",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_posts = WordCloud(\n",
    "        background_color = 'white',\n",
    "        stopwords = posts_stopwords,\n",
    "        mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "347d34da-e0a4-4fa7-bd75-0aa5524010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_comments = WordCloud(\n",
    "        background_color = 'white', \n",
    "        stopwords = comments_stopwords, \n",
    "        mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18be5dbe-ca56-45eb-a28d-1a6c1a48a44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1830838ea60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.generate_from_frequencies(posts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d86d94b7-ccd4-4629-a6fc-7524abc25af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1830836dfd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.generate_from_frequencies(comments_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac000ec1-d3c5-42bc-b715-1a8d09e3ad3a",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f2d3ed4-0a6b-4265-a29a-eb729dc897b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1830838ea60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.to_file('wc_post_single_round.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "058ab404-220c-41dc-8c3c-ccf966cb0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1830836dfd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.to_file('wc_comment_single_round.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ee84b6-6413-4330-90ad-fd00d889dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kevin\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee4c2f-f679-4ec4-9637-03b5927a2c91",
   "metadata": {},
   "source": [
    "# Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6688c4-ad5b-4b10-8b6c-3c3b75ccd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9d96d6-b227-4fea-a080-8f7d69836f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"C:/Users/kevin/OneDrive/Winter Case Competition/RedditIPO-SentimentAnalysis/Data\"\n",
    "os.chdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e06b1a6-fa33-411a-b12f-cd19653d82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading text\n",
    "posts = pd.read_csv('posts_rd.csv')\n",
    "comments = pd.read_csv('comments_rd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5cfc738-022f-457f-87ed-870e7d70edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.ogrid[:600, :600]\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae558daa-da08-493b-8b84-dd900c6b7048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_link_id</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hor72d8</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>Deleting all comments and accounts in 3... 2.....</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>4</td>\n",
       "      <td>stockmarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>horfp4g</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>Make the first offering to the users bitch</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>2</td>\n",
       "      <td>stockmarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>horc0ki</td>\n",
       "      <td>t1_hor72d8</td>\n",
       "      <td>You joke. But, just another social media site ...</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>2</td>\n",
       "      <td>stockmarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>horebg5</td>\n",
       "      <td>t1_horc0ki</td>\n",
       "      <td>I only joke for now. Once it really happens, I...</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>2</td>\n",
       "      <td>stockmarket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>hp8mmwv</td>\n",
       "      <td>t1_horebg5</td>\n",
       "      <td>We already are</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>1</td>\n",
       "      <td>stockmarket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1 comment_id comment_parent_id  \\\n",
       "0           0             0               0    hor72d8         t3_rhlfnx   \n",
       "1           1             1               1    horfp4g         t3_rhlfnx   \n",
       "2           2             2               2    horc0ki        t1_hor72d8   \n",
       "3           3             3               3    horebg5        t1_horc0ki   \n",
       "4           4             4               4    hp8mmwv        t1_horebg5   \n",
       "\n",
       "                                        comment_body comment_link_id  \\\n",
       "0  Deleting all comments and accounts in 3... 2.....       t3_rhlfnx   \n",
       "1         Make the first offering to the users bitch       t3_rhlfnx   \n",
       "2  You joke. But, just another social media site ...       t3_rhlfnx   \n",
       "3  I only joke for now. Once it really happens, I...       t3_rhlfnx   \n",
       "4                                     We already are       t3_rhlfnx   \n",
       "\n",
       "   comment_score comment_subreddit  \n",
       "0              4       stockmarket  \n",
       "1              2       stockmarket  \n",
       "2              2       stockmarket  \n",
       "3              2       stockmarket  \n",
       "4              1       stockmarket  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083c82f5-73e6-48cb-a5fc-308cd2f65b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stockmarket',\n",
       " 'HailCorporate',\n",
       " 'wallstreetbets',\n",
       " 'unclebens',\n",
       " 'GME',\n",
       " 'preppers',\n",
       " 'Bogleheads',\n",
       " 'antiwork',\n",
       " 'mauerstrassenwetten',\n",
       " 'IPO',\n",
       " 'superstonk',\n",
       " 'Wallstreetbetsnew',\n",
       " 'Infinity_For_Reddit',\n",
       " 'conspiracy',\n",
       " 'investing',\n",
       " 'OutOfTheLoop',\n",
       " 'UrvinFinance',\n",
       " 'stocks',\n",
       " 'CryptoCurrency',\n",
       " 'AskReddit',\n",
       " 'mademesmile',\n",
       " 'collapse']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of subreddits\n",
    "sublist = list(comments['comment_subreddit'].unique())\n",
    "sublist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45099146-fb36-416a-bc26-d9cc0cbed3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dict = {sub: comments[comments['comment_subreddit'] == sub] for sub in sublist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9327fcb7-fb8a-4165-a70c-39f2522fe41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [y for x, y in comments.groupby('comment_subreddit', as_index=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152802b-02b7-4433-9aaa-14f139d9bd8c",
   "metadata": {},
   "source": [
    "# Trim dataframe to list & dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f371e7b3-de15-47e6-9bfa-40235df1b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "comments_trimm = {}\n",
    "while i < 22:  \n",
    "    comments_trimm[i] = d[i][['comment_body']]\n",
    "    comments_trimm[i] = comments_trimm[i].to_string()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c27289-46df-4686-b293-d94e1e1e547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_trimm = posts[['title', 'body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db800718-bfc7-48fe-8a99-f9ccbb1bfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts\n",
    "posts_trimm = posts_trimm.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "161d7576-d33e-43ee-9b88-2aec723375cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-cdd15084e9a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomments_trimm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'head'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c26052c-a7b2-4964-9eae-293ff36666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonkenizer. Lemmatizer is used here to achieve full morphological analysis and accurately identify the lemma for each word, this is better than simply stemming. \n",
    "WNL = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d7275-b490-4652-a524-8b997f03b40b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc694c45-a1ed-47b9-aebb-fa557e657860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up texts by transforming to lower case, also removing apostrophe\n",
    "def lowerRep(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text\n",
    "\n",
    "# removing number\n",
    "def removeNum(text):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    return text\n",
    "\n",
    "# remove extra chars and stop words\n",
    "def removeChar(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text1 = nltk.Text(tokens)\n",
    "    text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text1]\n",
    "    return text_content\n",
    "\n",
    "# remove URLs and images\n",
    "def removeURL(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# remove ![img] string\n",
    "def removeImg(text):\n",
    "    text = re.sub(r'!\\[img]\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# initial removal of stopwords and empty spaces\n",
    "def removeSpace(text, stopwords):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    text = [s for s in text if len(s) != 0]\n",
    "    return text\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(text):\n",
    "    text = [WNL.lemmatize(t) for t in text]\n",
    "    return text\n",
    "\n",
    "# tokenize\n",
    "def tokenize(text):\n",
    "    tokens = [nltk.word_tokenize(i) for i in text]\n",
    "    return tokens\n",
    "\n",
    "# sigle word dictionary\n",
    "def singleDict(text):\n",
    "    dictionary = [' '.join(tup) for tup in text]\n",
    "    return dictionary\n",
    "\n",
    "# generate words frequency\n",
    "def wordsFreq(text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag_of_words = vectorizer.fit_transform(text)\n",
    "    vectorizer.vocabulary_\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c2442-d0eb-498e-8eed-d8ed7d20c750",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "564b22bc-87b7-4032-8bbf-fcbadd6f6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "posts_stopwords = list(STOPWORDS) + [\"https\", \"png\", \"imgur\", \"n\", \"click\", \"reddit\", \"ashx\", \"will\", \"CHART\", \n",
    "                                     \"t\", \"ta\", \"st_c\", \"sma\", \"smsch_200p\", \"bb_20_2\", \"webp\", \"s\", \"l\", \n",
    "                                    \"stofu_b_14_3_3\", \"macd_b_12_26_9\", \"rsi_b_14\", \"sch_200p\", \"p\", \"d\", \"c\", \"nn\"]\n",
    "comments_stopwords = list(STOPWORDS) + [\"n\", \"nbsp\", \"http\", \"u\", \"s\", \"reddit\", \"will\", \"tth\", \"emote\", \"nn\", \"commentbody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f388a2-87c8-4db6-b5c9-c98a44faf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# string1 = \"![img](emote|t5_2th52|4258)\"\n",
    "# string2 = \"https://github.com/yyd859/RedditIPO-SentimentAnalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa3af9ef-5050-4ad8-a8cc-69e7ca3432ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeImg(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ed97da3-d10e-4e4a-ad2c-f6662d86cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removeURL(string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62a37d-6898-4331-aead-6260580d444a",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72338282-e765-4f81-9c95-b46e57d8a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for posts\n",
    "posts_trimm = removeURL(posts_trimm)\n",
    "posts_trimm = removeImg(posts_trimm)\n",
    "posts_trimm = lowerRep(posts_trimm)\n",
    "posts_trimm = removeNum(posts_trimm)\n",
    "posts_content = removeChar(posts_trimm)\n",
    "posts_content = removeSpace(posts_content, posts_stopwords)\n",
    "posts_content = lemmatize(posts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc7cba-57be-4301-b593-d600ca8969f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee5ec200-ebc9-48c1-a49f-55d83f6fb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for comments: this should take longer\n",
    "i = 0\n",
    "while i < 22:  \n",
    "    comments_trimm[i] = removeURL(comments_trimm[i])\n",
    "    comments_trimm[i] = removeImg(comments_trimm[i])\n",
    "    comments_trimm[i] = lowerRep(comments_trimm[i])\n",
    "    comments_trimm[i] = removeNum(comments_trimm[i])\n",
    "    comments_trimm[i] = removeChar(comments_trimm[i])\n",
    "    comments_trimm[i] = removeSpace(comments_trimm[i], comments_stopwords)\n",
    "    comments_trimm[i] = lemmatize(comments_trimm[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343c74f-607d-4980-b5dd-f42a6e7f24ba",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c25bb2-a816-41d6-b76c-87348b04e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate dictionary for posts\n",
    "posts_token = tokenize(posts_content)\n",
    "# text2 = nltk.Text(posts_token)\n",
    "posts_single = singleDict(nltk.Text(posts_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a20f260b-1b23-4f38-956a-7d0b2458fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate dictionary for comments\n",
    "comments_token = {}\n",
    "comments_single = {}\n",
    "comments_word_freq = {}\n",
    "comments_dict = {}\n",
    "i = 0\n",
    "while i < 22:\n",
    "    comments_token[i] = tokenize(comments_trimm[i])\n",
    "    comments_single[i] = singleDict(nltk.Text(comments_token[i]))\n",
    "    comments_word_freq[i] = wordsFreq(comments_single[i])\n",
    "    comments_dict[i] = dict(comments_word_freq[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9714f92e-24fe-4d57-bf43-02432ae82558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "# print(posts_token)\n",
    "# print(posts_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f0f0393-5fa7-485f-b967-1658edb8a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for posts\n",
    "posts_word_freq = wordsFreq(posts_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c41bfb68-e406-4838-a446-46bb659fd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test block\n",
    "# print(posts_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16b133e8-247c-4608-9053-af8cf36c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for comments\n",
    "# comments_word_freq = wordsFreq(comments_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8eeaa137-c913-4dd3-88fa-00d4e3972bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posts_word_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-0ff18283d43b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mposts_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposts_word_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcomments_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments_word_freq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'posts_word_freq' is not defined"
     ]
    }
   ],
   "source": [
    "posts_dict = dict(posts_word_freq)\n",
    "# comments_dict = dict(comments_word_freq[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74313112-508d-49a2-a2cb-98317c7e3382",
   "metadata": {},
   "source": [
    "# Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca914e16-3e74-48b7-a5fb-a5eb2f881785",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_posts = WordCloud(\n",
    "        background_color = 'white',\n",
    "        stopwords = posts_stopwords,\n",
    "        mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "347d34da-e0a4-4fa7-bd75-0aa5524010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_comments = WordCloud(\n",
    "        background_color = 'white', \n",
    "        stopwords = comments_stopwords, \n",
    "        mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18be5dbe-ca56-45eb-a28d-1a6c1a48a44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x230b1d20100>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.generate_from_frequencies(posts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "176eb121-d10b-4dcd-94b8-72593d82d3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1a076583580>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.generate_from_frequencies(comments_dict[3])\n",
    "wc_comments.to_file('wc_comment_single_round_sub3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d86d94b7-ccd4-4629-a6fc-7524abc25af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 22:\n",
    "    wc_comments.generate_from_frequencies(comments_dict[i])\n",
    "#    wc_comments.to_file('wc_comment_single_sub_$s') %i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac000ec1-d3c5-42bc-b715-1a8d09e3ad3a",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f2d3ed4-0a6b-4265-a29a-eb729dc897b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x230b1d20100>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.to_file('wc_post_single_round.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "058ab404-220c-41dc-8c3c-ccf966cb0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x230b1d207f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.to_file('wc_comment_single_round.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c72c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (7.5.0)\n",
      "Requirement already satisfied: pandas in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: Datetime in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (4.3)\n",
      "Requirement already satisfied: pytz in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from Datetime) (2021.1)\n",
      "Requirement already satisfied: zope.interface in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from Datetime) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (1.2.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from prawcore<3,>=2.1->praw) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
      "Requirement already satisfied: setuptools in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from zope.interface->Datetime) (52.0.0.post20210125)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw pandas Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a950b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psaw in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (0.1.0)\r\n",
      "Requirement already satisfied: Click in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from psaw) (7.1.2)\r\n",
      "Requirement already satisfied: requests in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from psaw) (2.25.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (1.26.4)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests->psaw) (4.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install psaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7556319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# IMPORT PACKAGES\n",
    "#######\n",
    "\n",
    "import numpy as np\n",
    "import praw\n",
    "import pandas as pd\n",
    "import praw\n",
    "import datetime as dt\n",
    "from psaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0dfeb",
   "metadata": {},
   "source": [
    "This dataset gathered data from Reddit using praw (The Python Reddit API Wrapper).\n",
    "The API connection process is conducted under the guidence from the link below:\n",
    "\n",
    "https://praw.readthedocs.io/en/latest/getting_started/quick_start.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325e70a",
   "metadata": {},
   "source": [
    "Your username is: yyd859\n",
    "Your password is: 54804839\n",
    "Your app's client ID is: XRBFUnVyp6VVdknjdodIKQ\n",
    "Your app's client secret is: rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434907f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=\"XRBFUnVyp6VVdknjdodIKQ\",#my client id\n",
    "                     client_secret=\"rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ\",  #your client secret\n",
    "                     user_agent=\"Mac:yyd859.myredditapp:v0.1 (by /u/yyd859)\", #user agent name\n",
    "                     username = \"yyd859\",     # your reddit username\n",
    "                     password = \"54804839\")     # your reddit password\n",
    "# Use PushshiftAPI\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a105ec3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1612067964.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Acessing the reddit api\n",
    "s = 'wallstreetbets'   # make a list of subreddits you want to scrape the data from\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
    "########################################\n",
    "\n",
    "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
    "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
    "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
    "\n",
    "# SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc} we will go with keyword strategy i.e using search a keyword\n",
    "query = 'GME'\n",
    "#search by relevance\n",
    "comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_score\":[],\n",
    "            \"comment_subreddit\":[],\n",
    "            \"comment_time_stamp\":[],\n",
    "            \"post_time_stamp\":[]\n",
    "        } \n",
    "\n",
    "\n",
    "##1\n",
    "end_epoch_1=int(dt.datetime(2021, 1, 31).timestamp())\n",
    "sub=api.search_submissions(q=query,\n",
    "                            before=end_epoch_1,\n",
    "                            sort=\"desc\",\n",
    "                            subreddit=s,\n",
    "                            limit=100)\n",
    "            ##### Acessing comments on the post\n",
    "for submission in sub:\n",
    "    submission.comments.replace_more(limit = 1)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_dict[\"comment_id\"].append(comment.id)\n",
    "        comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "        comments_dict[\"comment_body\"].append(comment.body)\n",
    "        comments_dict[\"comment_score\"].append(comment.score)\n",
    "        comments_dict[\"comment_subreddit\"].append(s)\n",
    "        comments_dict[\"comment_time_stamp\"].append(comment.created_utc)\n",
    "        comments_dict[\"post_time_stamp\"].append(submission.created_utc)\n",
    "end_epoch_min=min(comments_dict[\"post_time_stamp\"])\n",
    "print(end_epoch_min)\n",
    "post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "post_comments.to_csv(s+\"_comments_before_\"+ query +\"_1\"+\"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a1cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1612066987.0\n"
     ]
    }
   ],
   "source": [
    "comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_score\":[],\n",
    "            \"comment_subreddit\":[],\n",
    "            \"comment_time_stamp\":[],\n",
    "            \"post_time_stamp\":[]\n",
    "        } \n",
    "end_epoch_1=1612067963\n",
    "sub=api.search_submissions(q=query,\n",
    "                            before=end_epoch_1,\n",
    "                            sort=\"desc\",\n",
    "                            subreddit=s,\n",
    "                            limit=100)\n",
    "            ##### Acessing comments on the post\n",
    "for submission in sub:\n",
    "    submission.comments.replace_more(limit = 1)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_dict[\"comment_id\"].append(comment.id)\n",
    "        comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "        comments_dict[\"comment_body\"].append(comment.body)\n",
    "        comments_dict[\"comment_score\"].append(comment.score)\n",
    "        comments_dict[\"comment_subreddit\"].append(s)\n",
    "        comments_dict[\"comment_time_stamp\"].append(comment.created_utc)\n",
    "        comments_dict[\"post_time_stamp\"].append(submission.created_utc)\n",
    "end_epoch_min=min(comments_dict[\"post_time_stamp\"])\n",
    "print(end_epoch_min)\n",
    "post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "post_comments.to_csv(s+\"_comments_before_\"+ query +\"_2\"+\"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d9dff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply merge\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = \"/Users/yyd/Documents/GitHub/RedditIPO-SentimentAnalysis/Code\"\n",
    "\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_comments_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccddee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate\n",
    "comments_raw=pd.read_csv(\"/Users/yyd/Documents/GitHub/RedditIPO-SentimentAnalysis/Code/comments.csv\")\n",
    "comments_rd=comments_raw.drop_duplicates(subset='comment_id')\n",
    "comments_rd.to_csv( \"comments_rd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "    post_comments.to_csv(s+\"_comments_relevance_\"+ query +\"_subreddit.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

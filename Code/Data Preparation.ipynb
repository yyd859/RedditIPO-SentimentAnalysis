{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c72c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (7.5.0)\n",
      "Requirement already satisfied: pandas in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: Datetime in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (4.3)\n",
      "Requirement already satisfied: zope.interface in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from Datetime) (5.3.0)\n",
      "Requirement already satisfied: pytz in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from Datetime) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (1.2.3)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from prawcore<3,>=2.1->praw) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2020.12.5)\n",
      "Requirement already satisfied: setuptools in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from zope.interface->Datetime) (52.0.0.post20210125)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw pandas Datetime psaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7556319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# IMPORT PACKAGES\n",
    "#######\n",
    "\n",
    "import numpy as np\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from psaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0dfeb",
   "metadata": {},
   "source": [
    "This dataset gathered data from Reddit using praw (The Python Reddit API Wrapper).\n",
    "The API connection process is conducted under the guidence from the link below:\n",
    "\n",
    "https://praw.readthedocs.io/en/latest/getting_started/quick_start.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325e70a",
   "metadata": {},
   "source": [
    "Your username is: yyd859\n",
    "Your password is: 54804839\n",
    "Your app's client ID is: XRBFUnVyp6VVdknjdodIKQ\n",
    "Your app's client secret is: rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "434907f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"%%\",\n",
    "    client_secret=\"%%\",\n",
    "    user_agent=\"%%\",\n",
    ")\n",
    "#Connection Check\n",
    "print(reddit.read_only)\n",
    "# Use PushshiftAPI\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf2269",
   "metadata": {},
   "source": [
    "#Data for Reddit IPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1a105ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Acessing the reddit api\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"%%\",#my client id\n",
    "                     client_secret=\"%%\",  #your client secret\n",
    "                     user_agent=\"%%\", #user agent name\n",
    "                     username = \"%%\",     # your reddit username\n",
    "                     password = \"%%\")     # your reddit password\n",
    "\n",
    "sub = ['wallstreetbets',\n",
    "'investing',\n",
    "'personalfinance',\n",
    "'stocks',\n",
    "'stockmarket',\n",
    "'superstonk',\n",
    "'antiwork',\n",
    "'mademesmile',\n",
    "'IPO',\n",
    "'AskReddit',\n",
    "'Wallstreetbetsnew',\n",
    "'CryptoCurrency',\n",
    "'conspiracy',\n",
    "'UrvinFinance',\n",
    "'Infinity_For_Reddit',\n",
    "'OutOfTheLoop',\n",
    "'collapse',\n",
    "'GME',\n",
    "'mauerstrassenwetten',\n",
    "'unclebens',\n",
    "'HailCorporate',\n",
    "'Bogleheads',\n",
    "'preppers']  # make a list of subreddits you want to scrape the data from\n",
    "\n",
    "\n",
    "for s in sub:\n",
    "    subreddit = reddit.subreddit(s)   # Chosing the subreddit\n",
    "\n",
    "\n",
    "########################################\n",
    "#   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
    "########################################\n",
    "\n",
    "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
    "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
    "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
    "\n",
    "# SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc} we will go with keyword strategy i.e using search a keyword\n",
    "    query = ['reddit IPO']\n",
    "#search by relevance\n",
    "    for item in query:\n",
    "        post_dict = {\n",
    "            \"title\" : [],\n",
    "            \"score\" : [],\n",
    "            \"id\" : [],\n",
    "            \"url\" : [],\n",
    "            \"comms_num\": [],\n",
    "            \"created\" : [],\n",
    "            \"body\" : []\n",
    "        }\n",
    "        comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_link_id\" : [],\n",
    "            \"comment_score\":[],\n",
    "            \"comment_subreddit\":[],\n",
    "            \"comment_time_stamp\":[]\n",
    "        }\n",
    "        for submission in subreddit.search(query,sort='relevance',time_filter=\"year\",limit = 50):\n",
    "            post_dict[\"title\"].append(submission.title)\n",
    "            post_dict[\"score\"].append(submission.score)\n",
    "            post_dict[\"id\"].append(submission.id)\n",
    "            post_dict[\"url\"].append(submission.url)\n",
    "            post_dict[\"comms_num\"].append(submission.num_comments)\n",
    "            post_dict[\"created\"].append(submission.created)\n",
    "            post_dict[\"body\"].append(submission.selftext)\n",
    "            \n",
    "            ##### Acessing comments on the post\n",
    "            submission.comments.replace_more(limit = 1)\n",
    "            for comment in submission.comments.list():\n",
    "                comments_dict[\"comment_id\"].append(comment.id)\n",
    "                comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                comments_dict[\"comment_body\"].append(comment.body)\n",
    "                comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "                comments_dict[\"comment_score\"].append(comment.score)\n",
    "                comments_dict[\"comment_subreddit\"].append(s)\n",
    "                comments_dict[\"comment_time_stamp\"].append(comment.created_utc)\n",
    "        \n",
    "        post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "        post_comments.to_csv(s+\"_comments_relevance_\"+ item +\"_subreddit.csv\")\n",
    "        post_data = pd.DataFrame(post_dict)\n",
    "        post_data.to_csv(s+\"_posts_relevance\"+ item +\"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "08ce4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sub:\n",
    "    subreddit = reddit.subreddit(s)\n",
    "    query = ['reddit IPO']\n",
    "#search by top\n",
    "    for item in query:\n",
    "        post_dict = {\n",
    "            \"title\" : [],\n",
    "            \"score\" : [],\n",
    "            \"id\" : [],\n",
    "            \"url\" : [],\n",
    "            \"comms_num\": [],\n",
    "            \"created\" : [],\n",
    "            \"body\" : []\n",
    "        }\n",
    "        comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_link_id\" : [],\n",
    "            \"comment_score\":[],\n",
    "            \"comment_subreddit\":[],\n",
    "            \"comment_time_stamp\":[]\n",
    "        }\n",
    "        for submission in subreddit.search(query,sort='top',time_filter=\"year\",limit = 50):\n",
    "            post_dict[\"title\"].append(submission.title)\n",
    "            post_dict[\"score\"].append(submission.score)\n",
    "            post_dict[\"id\"].append(submission.id)\n",
    "            post_dict[\"url\"].append(submission.url)\n",
    "            post_dict[\"comms_num\"].append(submission.num_comments)\n",
    "            post_dict[\"created\"].append(submission.created)\n",
    "            post_dict[\"body\"].append(submission.selftext)\n",
    "            \n",
    "            ##### Acessing comments on the post\n",
    "            submission.comments.replace_more(limit = 1)\n",
    "            for comment in submission.comments.list():\n",
    "                comments_dict[\"comment_id\"].append(comment.id)\n",
    "                comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                comments_dict[\"comment_body\"].append(comment.body)\n",
    "                comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "                comments_dict[\"comment_score\"].append(comment.score)\n",
    "                comments_dict[\"comment_subreddit\"].append(s)\n",
    "                comments_dict[\"comment_time_stamp\"].append(comment.created_utc)\n",
    "        \n",
    "        post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "        post_comments.to_csv(s+\"_comments_top_\"+ item +\"_subreddit.csv\")\n",
    "        post_data = pd.DataFrame(post_dict)\n",
    "        post_data.to_csv(s+\"_posts_top\"+ item +\"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d9dff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply merge\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path =\"%%\"\n",
    "\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_comments_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"comments.csv\")\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_posts_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"posts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ccddee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate\n",
    "comments_raw=pd.read_csv(\"%%/comments.csv\")\n",
    "posts_raw=pd.read_csv(\"%%/posts.csv\")\n",
    "comments_rd=comments_raw.drop_duplicates(subset='comment_id')\n",
    "posts_rd=posts_raw.drop_duplicates(subset='id')\n",
    "comments_rd.to_csv( \"comments_reddit.csv\")\n",
    "posts_rd.to_csv( \"posts_rd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6dde1",
   "metadata": {},
   "source": [
    "#Data for GME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18371392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Acessing the reddit api\n",
    "s = 'wallstreetbets'   # make a list of subreddits you want to scrape the data from\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "#   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
    "########################################\n",
    "\n",
    "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
    "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
    "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
    "\n",
    "# SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc} we will go with keyword strategy i.e using search a keyword\n",
    "query = 'GME'\n",
    "#search by relevance\n",
    "comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_score\":[],\n",
    "            \"comment_subreddit\":[],\n",
    "            \"comment_time_stamp\":[],\n",
    "            \"post_time_stamp\":[]\n",
    "        } \n",
    "\n",
    "\n",
    "##1\n",
    "start_epoch_1=int(dt.datetime(2020, 8, 1).timestamp())\n",
    "sub=api.search_submissions(q=query,\n",
    "                            after=start_epoch_1,\n",
    "                            sort=\"asc\",\n",
    "                            subreddit=s,\n",
    "                            limit=100)\n",
    "            ##### Acessing comments on the post\n",
    "for submission in sub:\n",
    "    submission.comments.replace_more(limit = 1)\n",
    "    for comment in submission.comments.list():\n",
    "        comments_dict[\"comment_id\"].append(comment.id)\n",
    "        comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "        comments_dict[\"comment_body\"].append(comment.body)\n",
    "        comments_dict[\"comment_score\"].append(comment.score)\n",
    "        comments_dict[\"comment_subreddit\"].append(s)\n",
    "        comments_dict[\"comment_time_stamp\"].append(comment.created_utc)\n",
    "        comments_dict[\"post_time_stamp\"].append(submission.created_utc)\n",
    "start_epoch_max=max(comments_dict[\"post_time_stamp\"])\n",
    "print(start_epoch_max)\n",
    "post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "post_comments.to_csv(s+\"_comments_\"+ query +\"_1\"+\"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply merge\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = \"/Users/yyd/Documents/GitHub/RedditIPO-SentimentAnalysis/Code\"\n",
    "\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_comments_GME_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f69042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate\n",
    "comments_raw=pd.read_csv(\"/Users/yyd/Documents/GitHub/RedditIPO-SentimentAnalysis/Code/comments.csv\")\n",
    "comments_rd=comments_raw.drop_duplicates(subset='comment_id')\n",
    "comments_rd.to_csv( \"comments_GME.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcac447",
   "metadata": {},
   "source": [
    "#Final Data Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Data Writing\n",
    "comments_reddit= pd.read_csv('comments_reddit.csv')\n",
    "comments_GME=pd.read_csv('comments_GME.csv')\n",
    "comments_reddit['comment_time_stamp'] = comments_reddit['comment_time_stamp'].apply(datetime.fromtimestamp)\n",
    "comments_GME['comment_time_stamp']=comments_GME['comment_time_stamp'].apply(datetime.fromtimestamp)\n",
    "comments_GME['post_time_stamp']=comments_GME['post_time_stamp'].apply(datetime.fromtimestamp)\n",
    "comments_reddit.to_csv(\"comments_reddit_1.csv\")\n",
    "comments_GME.to_csv(\"comments_GME_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

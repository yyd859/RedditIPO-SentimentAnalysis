{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c72c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (7.5.0)\n",
      "Requirement already satisfied: pandas in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: datetime in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (4.3)\n",
      "Requirement already satisfied: tqdm in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (4.59.0)\n",
      "Requirement already satisfied: pytz in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from datetime) (2021.1)\n",
      "Requirement already satisfied: zope.interface in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from datetime) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (1.2.3)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from prawcore<3,>=2.1->praw) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from zope.interface->datetime) (52.0.0.post20210125)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0dfeb",
   "metadata": {},
   "source": [
    "This dataset gathered data from Reddit using praw (The Python Reddit API Wrapper).\n",
    "The API connection process is conducted under the guidence from the link below:\n",
    "\n",
    "https://praw.readthedocs.io/en/latest/getting_started/quick_start.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325e70a",
   "metadata": {},
   "source": [
    "Your username is: yyd859\n",
    "Your password is: 54804839\n",
    "Your app's client ID is: XRBFUnVyp6VVdknjdodIKQ(不确定是不是这个)\n",
    "Your app's client secret is: rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "434907f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"XRBFUnVyp6VVdknjdodIKQ\",\n",
    "    client_secret=\"rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ\",\n",
    "    user_agent=\"Mac:yyd859.myredditapp:v0.1 (by /u/yyd859)\",\n",
    ")\n",
    "#Connection Check\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1a105ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# IMPORT PACKAGES\n",
    "#######\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Acessing the reddit api\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"XRBFUnVyp6VVdknjdodIKQ\",#my client id\n",
    "                     client_secret=\"rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ\",  #your client secret\n",
    "                     user_agent=\"Mac:yyd859.myredditapp:v0.1 (by /u/yyd859)\", #user agent name\n",
    "                     username = \"yyd859\",     # your reddit username\n",
    "                     password = \"54804839\")     # your reddit password\n",
    "\n",
    "sub = ['wallstreetbets',\n",
    "'investing',\n",
    "'personalfinance',\n",
    "'stocks',\n",
    "'stockmarket',\n",
    "'superstonk',\n",
    "'antiwork',\n",
    "'mademesmile',\n",
    "'IPO',\n",
    "'AskReddit',\n",
    "'Wallstreetbetsnew',\n",
    "'CryptoCurrency',\n",
    "'conspiracy',\n",
    "'UrvinFinance',\n",
    "'Infinity_For_Reddit',\n",
    "'OutOfTheLoop',\n",
    "'collapse',\n",
    "'GME',\n",
    "'mauerstrassenwetten',\n",
    "'unclebens',\n",
    "'HailCorporate',\n",
    "'Bogleheads',\n",
    "'preppers']  # make a list of subreddits you want to scrape the data from\n",
    "\n",
    "\n",
    "for s in sub:\n",
    "    subreddit = reddit.subreddit(s)   # Chosing the subreddit\n",
    "\n",
    "\n",
    "########################################\n",
    "#   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
    "########################################\n",
    "\n",
    "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
    "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
    "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
    "\n",
    "# SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc} we will go with keyword strategy i.e using search a keyword\n",
    "    query = ['reddit IPO']\n",
    "#search by relevance\n",
    "    for item in query:\n",
    "        post_dict = {\n",
    "            \"title\" : [],\n",
    "            \"score\" : [],\n",
    "            \"id\" : [],\n",
    "            \"url\" : [],\n",
    "            \"comms_num\": [],\n",
    "            \"created\" : [],\n",
    "            \"body\" : []\n",
    "        }\n",
    "        comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_link_id\" : []\n",
    "        }\n",
    "        for submission in subreddit.search(query,sort='relevance',time_filter=\"month\",limit = 20):\n",
    "            post_dict[\"title\"].append(submission.title)\n",
    "            post_dict[\"score\"].append(submission.score)\n",
    "            post_dict[\"id\"].append(submission.id)\n",
    "            post_dict[\"url\"].append(submission.url)\n",
    "            post_dict[\"comms_num\"].append(submission.num_comments)\n",
    "            post_dict[\"created\"].append(submission.created)\n",
    "            post_dict[\"body\"].append(submission.selftext)\n",
    "            \n",
    "            ##### Acessing comments on the post\n",
    "            submission.comments.replace_more(limit = 1)\n",
    "            for comment in submission.comments.list():\n",
    "                comments_dict[\"comment_id\"].append(comment.id)\n",
    "                comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                comments_dict[\"comment_body\"].append(comment.body)\n",
    "                comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "        \n",
    "        post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "        post_comments.to_csv(s+\"_comments_relevance_\"+ item +\"_subreddit.csv\")\n",
    "        post_data = pd.DataFrame(post_dict)\n",
    "        post_data.to_csv(s+\"_posts_relevance\"+ item +\"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "08ce4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sub:\n",
    "    subreddit = reddit.subreddit(s)\n",
    "    query = ['reddit IPO']\n",
    "#search by top\n",
    "    for item in query:\n",
    "        post_dict = {\n",
    "            \"title\" : [],\n",
    "            \"score\" : [],\n",
    "            \"id\" : [],\n",
    "            \"url\" : [],\n",
    "            \"comms_num\": [],\n",
    "            \"created\" : [],\n",
    "            \"body\" : []\n",
    "        }\n",
    "        comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_link_id\" : []\n",
    "        }\n",
    "        for submission in subreddit.search(query,sort='top',time_filter=\"month\",limit = 20):\n",
    "            post_dict[\"title\"].append(submission.title)\n",
    "            post_dict[\"score\"].append(submission.score)\n",
    "            post_dict[\"id\"].append(submission.id)\n",
    "            post_dict[\"url\"].append(submission.url)\n",
    "            post_dict[\"comms_num\"].append(submission.num_comments)\n",
    "            post_dict[\"created\"].append(submission.created)\n",
    "            post_dict[\"body\"].append(submission.selftext)\n",
    "            \n",
    "            ##### Acessing comments on the post\n",
    "            submission.comments.replace_more(limit = 1)\n",
    "            for comment in submission.comments.list():\n",
    "                comments_dict[\"comment_id\"].append(comment.id)\n",
    "                comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                comments_dict[\"comment_body\"].append(comment.body)\n",
    "                comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "        \n",
    "        post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "        post_comments.to_csv(s+\"_comments_top_\"+ item +\"_subreddit.csv\")\n",
    "        post_data = pd.DataFrame(post_dict)\n",
    "        post_data.to_csv(s+\"_posts_top\"+ item +\"_subreddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4d9dff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply merge\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = \"/Users/yyd/Documents/Duke Fuqua MQM/Jupyter Notebook/Formula1-RedditForum-SentimentAnalysis\"\n",
    "\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_comments_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"comments.csv\")\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_posts_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"posts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

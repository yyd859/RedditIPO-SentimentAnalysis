{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c72c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (7.5.0)\n",
      "Requirement already satisfied: pandas in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: Datetime in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (4.3)\n",
      "Collecting psraw\n",
      "  Using cached psraw-0.1.2.tar.gz (4.0 kB)\n",
      "Requirement already satisfied: zope.interface in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from Datetime) (5.3.0)\n",
      "Requirement already satisfied: pytz in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from Datetime) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (1.2.3)\n",
      "Requirement already satisfied: update-checker>=0.18 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from prawcore<3,>=2.1->praw) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yyd/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2020.12.5)\n",
      "Collecting praw\n",
      "  Using cached praw-5.4.0-py2.py3-none-any.whl (94 kB)\n",
      "  Using cached praw-5.3.0-py2.py3-none-any.whl (88 kB)\n",
      "  Using cached praw-5.2.0-py2.py3-none-any.whl (88 kB)\n",
      "  Using cached praw-5.1.0-py2.py3-none-any.whl (88 kB)\n",
      "  Using cached praw-5.0.1-py2.py3-none-any.whl (86 kB)\n",
      "  Using cached praw-5.0.0-py2.py3-none-any.whl (86 kB)\n",
      "  Using cached praw-4.6.0-py2.py3-none-any.whl (86 kB)\n",
      "  Using cached praw-4.5.1-py2.py3-none-any.whl (85 kB)\n",
      "  Using cached praw-4.5.0-py2.py3-none-any.whl (85 kB)\n",
      "  Using cached praw-4.4.0-py2.py3-none-any.whl (78 kB)\n",
      "  Using cached praw-4.3.0-py2.py3-none-any.whl (74 kB)\n",
      "  Using cached praw-4.2.0-py2.py3-none-any.whl (72 kB)\n",
      "Collecting decorator<4.1,>=4.0.9\n",
      "  Using cached decorator-4.0.11-py2.py3-none-any.whl (8.9 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.10.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting praw\n",
      "  Using cached praw-4.1.0-py2.py3-none-any.whl (71 kB)\n",
      "  Using cached praw-4.0.0-py2.py3-none-any.whl (64 kB)\n",
      "INFO: pip is looking at multiple versions of psraw to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting psraw\n",
      "  Using cached psraw-0.1.1.tar.gz (3.9 kB)\n",
      "  Using cached psraw-0.1.0.tar.gz (3.5 kB)\n",
      "  Using cached psraw-0.0.2.tar.gz (2.0 kB)\n",
      "  Using cached psraw-0.0.1.tar.gz (2.0 kB)\n",
      "INFO: pip is looking at multiple versions of websocket-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting websocket-client>=0.54.0\n",
      "  Using cached websocket_client-1.2.2-py3-none-any.whl (53 kB)\n",
      "  Using cached websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached websocket_client-1.2.0-py2.py3-none-any.whl (52 kB)\n",
      "  Using cached websocket_client-1.1.1-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached websocket_client-1.1.0-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached websocket_client-1.0.1-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached websocket_client-1.0.0-py2.py3-none-any.whl (68 kB)\n",
      "  Using cached websocket_client-0.59.0-py2.py3-none-any.whl (67 kB)\n",
      "  Using cached websocket_client-0.58.0-py2.py3-none-any.whl (61 kB)\n",
      "  Using cached websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "  Using cached websocket_client-0.56.0-py2.py3-none-any.whl (200 kB)\n",
      "  Using cached websocket_client-0.55.0-py2.py3-none-any.whl (200 kB)\n",
      "  Using cached websocket_client-0.54.0-py2.py3-none-any.whl (200 kB)\n",
      "INFO: pip is looking at multiple versions of psraw to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: pip is looking at multiple versions of websocket-client to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: pip is looking at multiple versions of urllib3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "  Using cached urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
      "  Using cached urllib3-1.26.3-py2.py3-none-any.whl (137 kB)\n",
      "  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)\n",
      "  Using cached urllib3-1.26.1-py2.py3-none-any.whl (136 kB)\n",
      "  Using cached urllib3-1.26.0-py2.py3-none-any.whl (136 kB)\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "  Using cached urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "  Using cached urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "  Using cached urllib3-1.25.7-py2.py3-none-any.whl (125 kB)\n",
      "  Using cached urllib3-1.25.6-py2.py3-none-any.whl (125 kB)\n",
      "  Using cached urllib3-1.25.5-py2.py3-none-any.whl (125 kB)\n",
      "  Using cached urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
      "  Using cached urllib3-1.25.3-py2.py3-none-any.whl (150 kB)\n",
      "  Using cached urllib3-1.25.2-py2.py3-none-any.whl (150 kB)\n",
      "  Using cached urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "  Using cached urllib3-1.24.2-py2.py3-none-any.whl (131 kB)\n",
      "  Using cached urllib3-1.24.1-py2.py3-none-any.whl (118 kB)\n",
      "  Using cached urllib3-1.24-py2.py3-none-any.whl (117 kB)\n",
      "  Using cached urllib3-1.23-py2.py3-none-any.whl (133 kB)\n",
      "  Using cached urllib3-1.22-py2.py3-none-any.whl (132 kB)\n",
      "  Using cached urllib3-1.21.1-py2.py3-none-any.whl (131 kB)\n",
      "INFO: pip is looking at multiple versions of urllib3 to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: pip is looking at multiple versions of update-checker to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "  Using cached idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "  Using cached idna-2.7-py2.py3-none-any.whl (58 kB)\n",
      "  Using cached idna-2.6-py2.py3-none-any.whl (56 kB)\n",
      "  Using cached idna-2.5-py2.py3-none-any.whl (55 kB)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install praw pandas Datetime psraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7556319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# IMPORT PACKAGES\n",
    "#######\n",
    "\n",
    "import numpy as np\n",
    "import praw\n",
    "import pandas as pd\n",
    "import praw\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0dfeb",
   "metadata": {},
   "source": [
    "This dataset gathered data from Reddit using praw (The Python Reddit API Wrapper).\n",
    "The API connection process is conducted under the guidence from the link below:\n",
    "\n",
    "https://praw.readthedocs.io/en/latest/getting_started/quick_start.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325e70a",
   "metadata": {},
   "source": [
    "Your username is: yyd859\n",
    "Your password is: 54804839\n",
    "Your app's client ID is: XRBFUnVyp6VVdknjdodIKQ\n",
    "Your app's client secret is: rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "434907f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"XRBFUnVyp6VVdknjdodIKQ\",\n",
    "    client_secret=\"rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ\",\n",
    "    user_agent=\"Mac:yyd859.myredditapp:v0.1 (by /u/yyd859)\",\n",
    ")\n",
    "#Connection Check\n",
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a105ec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TooLarge",
     "evalue": "received 413 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooLarge\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-920affb6e26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m##### Acessing comments on the post\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mcomments_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/praw/models/comment_forest.py\u001b[0m in \u001b[0;36mreplace_more\u001b[0;34m(self, limit, threshold)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mnew_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/praw/models/reddit/more.py\u001b[0m in \u001b[0;36mcomments\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;34m\"sort\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment_sort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             }\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"morechildren\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_comments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, data, files, params, json)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mattempts\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 return self._objectify_request(\n\u001b[0m\u001b[1;32m    794\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36m_objectify_request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \"\"\"\n\u001b[1;32m    695\u001b[0m         return self._objector.objectify(\n\u001b[0;32m--> 696\u001b[0;31m             self.request(\n\u001b[0m\u001b[1;32m    697\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At most one of `data` and `json` is supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             return self._core.request(\n\u001b[0m\u001b[1;32m    886\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         return self._request_with_retries(\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    264\u001b[0m             )\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTooLarge\u001b[0m: received 413 HTTP response"
     ]
    }
   ],
   "source": [
    "\n",
    "# Acessing the reddit api\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"XRBFUnVyp6VVdknjdodIKQ\",#my client id\n",
    "                     client_secret=\"rPqYL1OJMiKoek-7VGWOHoQvFbUfpQ\",  #your client secret\n",
    "                     user_agent=\"Mac:yyd859.myredditapp:v0.1 (by /u/yyd859)\", #user agent name\n",
    "                     username = \"yyd859\",     # your reddit username\n",
    "                     password = \"54804839\")     # your reddit password\n",
    "\n",
    "sub = ['wallstreetbets',\n",
    "'investing',\n",
    "'personalfinance',\n",
    "'stocks',\n",
    "'stockmarket',\n",
    "'superstonk',\n",
    "'antiwork',\n",
    "'mademesmile',\n",
    "'IPO',\n",
    "'AskReddit',\n",
    "'Wallstreetbetsnew',\n",
    "'CryptoCurrency',\n",
    "'conspiracy',\n",
    "'UrvinFinance',\n",
    "'Infinity_For_Reddit',\n",
    "'OutOfTheLoop',\n",
    "'collapse',\n",
    "'GME',\n",
    "'mauerstrassenwetten',\n",
    "'unclebens',\n",
    "'HailCorporate',\n",
    "'Bogleheads',\n",
    "'preppers']  # make a list of subreddits you want to scrape the data from\n",
    "\n",
    "\n",
    "for s in sub:\n",
    "    subreddit = reddit.subreddit(s)   # Chosing the subreddit\n",
    "\n",
    "\n",
    "########################################\n",
    "#   CREATING DICTIONARY TO STORE THE DATA WHICH WILL BE CONVERTED TO A DATAFRAME\n",
    "########################################\n",
    "\n",
    "#   NOTE: ALL THE POST DATA AND COMMENT DATA WILL BE SAVED IN TWO DIFFERENT\n",
    "#   DATASETS AND LATER CAN BE MAPPED USING IDS OF POSTS/COMMENTS AS WE WILL \n",
    "#   BE CAPTURING ALL IDS THAT COME IN OUR WAY\n",
    "\n",
    "# SCRAPING CAN BE DONE VIA VARIOUS STRATEGIES {HOT,TOP,etc} we will go with keyword strategy i.e using search a keyword\n",
    "    query = ['GME']\n",
    "#search by relevance\n",
    "    for item in query:\n",
    "        post_dict = {\n",
    "            \"title\" : [],\n",
    "            \"score\" : [],\n",
    "            \"id\" : [],\n",
    "            \"url\" : [],\n",
    "            \"comms_num\": [],\n",
    "            \"created\" : [],\n",
    "            \"body\" : []\n",
    "        }\n",
    "        comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_link_id\" : [],\n",
    "            \"comment_score\":[],\n",
    "            \"comment_subreddit\":[],\n",
    "            \"comment_time_stamp\":[]\n",
    "        }\n",
    "        for submission in subreddit.search(query,sort='relevance',time_filter=\"all\",limit = 200):\n",
    "            post_dict[\"title\"].append(submission.title)\n",
    "            post_dict[\"score\"].append(submission.score)\n",
    "            post_dict[\"id\"].append(submission.id)\n",
    "            post_dict[\"url\"].append(submission.url)\n",
    "            post_dict[\"comms_num\"].append(submission.num_comments)\n",
    "            post_dict[\"created\"].append(submission.created)\n",
    "            post_dict[\"body\"].append(submission.selftext)\n",
    "            \n",
    "            ##### Acessing comments on the post\n",
    "            submission.comments.replace_more(limit = 1)\n",
    "            for comment in submission.comments.list():\n",
    "                comments_dict[\"comment_id\"].append(comment.id)\n",
    "                comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                comments_dict[\"comment_body\"].append(comment.body)\n",
    "                comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "                comments_dict[\"comment_score\"].append(comment.score)\n",
    "                comments_dict[\"comment_subreddit\"].append(s)\n",
    "                comments_dict[\"comment_time_stamp\"].append(comment.created_utc)\n",
    "        \n",
    "        post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "        post_comments.to_csv(s+\"_comments_relevance_\"+ item +\"_subreddit_GME.csv\")\n",
    "        post_data = pd.DataFrame(post_dict)\n",
    "        post_data.to_csv(s+\"_posts_relevance\"+ item +\"_subreddit_GME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ce4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sub:\n",
    "    subreddit = reddit.subreddit(s)\n",
    "    query = ['GME']\n",
    "#search by top\n",
    "    for item in query:\n",
    "        post_dict = {\n",
    "            \"title\" : [],\n",
    "            \"score\" : [],\n",
    "            \"id\" : [],\n",
    "            \"url\" : [],\n",
    "            \"comms_num\": [],\n",
    "            \"created\" : [],\n",
    "            \"body\" : []\n",
    "        }\n",
    "        comments_dict = {\n",
    "            \"comment_id\" : [],\n",
    "            \"comment_parent_id\" : [],\n",
    "            \"comment_body\" : [],\n",
    "            \"comment_link_id\" : [],\n",
    "            \"comment_score\":[],\n",
    "            \"comment_subreddit\":[],\n",
    "            \"comment_time_stamp\":[]\n",
    "        }\n",
    "        for submission in subreddit.search(query,sort='top',time_filter=\"all\",limit = 500):\n",
    "            post_dict[\"title\"].append(submission.title)\n",
    "            post_dict[\"score\"].append(submission.score)\n",
    "            post_dict[\"id\"].append(submission.id)\n",
    "            post_dict[\"url\"].append(submission.url)\n",
    "            post_dict[\"comms_num\"].append(submission.num_comments)\n",
    "            post_dict[\"created\"].append(submission.created)\n",
    "            post_dict[\"body\"].append(submission.selftext)\n",
    "            \n",
    "            ##### Acessing comments on the post\n",
    "            submission.comments.replace_more(limit = 1)\n",
    "            for comment in submission.comments.list():\n",
    "                comments_dict[\"comment_id\"].append(comment.id)\n",
    "                comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                comments_dict[\"comment_body\"].append(comment.body)\n",
    "                comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "                comments_dict[\"comment_score\"].append(comment.score)\n",
    "                comments_dict[\"comment_subreddit\"].append(s)\n",
    "                comments_dict[\"comment_time_stamp\"].append(comment.created_utc)\n",
    "        \n",
    "        post_comments = pd.DataFrame(comments_dict)\n",
    "\n",
    "        post_comments.to_csv(s+\"_comments_top_\"+ item +\"_subreddit_GME.csv\")\n",
    "        post_data = pd.DataFrame(post_dict)\n",
    "        post_data.to_csv(s+\"_posts_top\"+ item +\"_subreddit_GME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9dff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply merge\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = \"/Users/yyd/Documents/GitHub/RedditIPO-SentimentAnalysis/Code\"\n",
    "\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_comments_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"comments_GME.csv\")\n",
    "\n",
    "comments_files = glob.glob(os.path.join(path, \"*_posts_*.csv\"))\n",
    "df_from_each_file = (pd.read_csv(f, sep=',') for f in comments_files)\n",
    "df_merged   = pd.concat(df_from_each_file, ignore_index=True)\n",
    "df_merged.to_csv( \"posts_GME.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccddee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate\n",
    "comments_raw=pd.read_csv(\"/Users/yyd/Documents/GitHub/RedditIPO-SentimentAnalysis/Code/comments_.csv\")\n",
    "posts_raw=pd.read_csv(\"/Users/yyd/Documents/GitHub/RedditIPO-SentimentAnalysis/Code/posts_.csv\")\n",
    "comments_rd=comments_raw.drop_duplicates(subset='comment_id')\n",
    "posts_rd=posts_raw.drop_duplicates(subset='id')\n",
    "comments_rd.to_csv( \"comments_rd_GME.csv\")\n",
    "posts_rd.to_csv( \"posts_rd_GME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2ccfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09046a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

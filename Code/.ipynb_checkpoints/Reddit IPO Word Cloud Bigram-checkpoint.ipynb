{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ee84b6-6413-4330-90ad-fd00d889dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kevin\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee4c2f-f679-4ec4-9637-03b5927a2c91",
   "metadata": {},
   "source": [
    "# Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6688c4-ad5b-4b10-8b6c-3c3b75ccd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500dd145-1d88-4f12-bc84-56713f69df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9d96d6-b227-4fea-a080-8f7d69836f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"C:/Users/kevin/OneDrive/Winter Case Competition/RedditIPO-SentimentAnalysis/Data\"\n",
    "os.chdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e06b1a6-fa33-411a-b12f-cd19653d82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading text\n",
    "posts = pd.read_csv('posts_rd.csv')\n",
    "comments = pd.read_csv('normalized_sentiment_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdeac124-ee5d-49b8-ab19-cfa766feb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.array(Image.open(\"reddit_icon.png\"))\n",
    "comments=comments[comments.sentiment_adjusted<0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d98bb9-3e1b-42f4-91eb-95bfde3bd69f",
   "metadata": {},
   "source": [
    "## A few check points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1592e4a-f31a-4f5c-b6ef-929fcc109cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(12/27) Monday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td>60</td>\n",
       "      <td>rpn6u5</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.640611e+09</td>\n",
       "      <td># Good Monday morning traders and investors of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(12/15) Wednesday's Pre-Market Stock Movers &amp; ...</td>\n",
       "      <td>25</td>\n",
       "      <td>rgzir5</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>17</td>\n",
       "      <td>1.639576e+09</td>\n",
       "      <td>#Good morning traders and investors of the r/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>(12/13) Monday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td>15</td>\n",
       "      <td>rffrcz</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>18</td>\n",
       "      <td>1.639402e+09</td>\n",
       "      <td># Good Monday morning traders and investors of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Reddit Files Confidentially for IPO. As of Aug...</td>\n",
       "      <td>15</td>\n",
       "      <td>rhlfnx</td>\n",
       "      <td>https://www.wsj.com/articles/reddit-files-pape...</td>\n",
       "      <td>16</td>\n",
       "      <td>1.639639e+09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>(12/14) Tuesday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td>13</td>\n",
       "      <td>rg793x</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>25</td>\n",
       "      <td>1.639488e+09</td>\n",
       "      <td>#Good morning traders and investors of the r/S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n",
       "0           0             0               0   \n",
       "1           1             1               1   \n",
       "2           2             2               2   \n",
       "3           3             3               3   \n",
       "4           4             4               4   \n",
       "\n",
       "                                               title  score      id  \\\n",
       "0    (12/27) Monday's Pre-Market Stock Movers & News     60  rpn6u5   \n",
       "1  (12/15) Wednesday's Pre-Market Stock Movers & ...     25  rgzir5   \n",
       "2    (12/13) Monday's Pre-Market Stock Movers & News     15  rffrcz   \n",
       "3  Reddit Files Confidentially for IPO. As of Aug...     15  rhlfnx   \n",
       "4   (12/14) Tuesday's Pre-Market Stock Movers & News     13  rg793x   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0  https://www.reddit.com/r/StockMarket/comments/...          8  1.640611e+09   \n",
       "1  https://www.reddit.com/r/StockMarket/comments/...         17  1.639576e+09   \n",
       "2  https://www.reddit.com/r/StockMarket/comments/...         18  1.639402e+09   \n",
       "3  https://www.wsj.com/articles/reddit-files-pape...         16  1.639639e+09   \n",
       "4  https://www.reddit.com/r/StockMarket/comments/...         25  1.639488e+09   \n",
       "\n",
       "                                                body  \n",
       "0  # Good Monday morning traders and investors of...  \n",
       "1  #Good morning traders and investors of the r/S...  \n",
       "2  # Good Monday morning traders and investors of...  \n",
       "3                                                NaN  \n",
       "4  #Good morning traders and investors of the r/S...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e30a86-fbd8-4f4e-9cf9-ee6ca002a6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_link_id</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_subreddit</th>\n",
       "      <th>fixed_body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_adjusted</th>\n",
       "      <th>subcomment_count</th>\n",
       "      <th>subcomment_weight</th>\n",
       "      <th>upvote_weight</th>\n",
       "      <th>weighted_sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hor72d8</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>Deleting all comments and accounts in 3... 2.....</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>4</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>Deleting all comments and accounts in 3... 2.....</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>-2.061675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>horc0ki</td>\n",
       "      <td>t1_hor72d8</td>\n",
       "      <td>You joke. But, just another social media site ...</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>2</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>You joke. But, just another social media site ...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>-1.030597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>horebg5</td>\n",
       "      <td>t1_horc0ki</td>\n",
       "      <td>I only joke for now. Once it really happens, I...</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>2</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>I only joke for now. Once it really happens, I...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>-2.061193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>hp8y40y</td>\n",
       "      <td>t1_hp8mmwv</td>\n",
       "      <td>In what way?</td>\n",
       "      <td>t3_rhlfnx</td>\n",
       "      <td>1</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>In what way?</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007056</td>\n",
       "      <td>-2.014113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>hosnjyl</td>\n",
       "      <td>t3_rhn77f</td>\n",
       "      <td>It will not have ther momentum of Donald Trump...</td>\n",
       "      <td>t3_rhn77f</td>\n",
       "      <td>-2</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>It will not have ther momentum of Donald Trump...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>-1.006704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0           0             0               0                 0   \n",
       "2           2             2               2                 2   \n",
       "3           3             3               3                 3   \n",
       "5           5             5               5                 5   \n",
       "8           8             8               8                 8   \n",
       "\n",
       "   Unnamed: 0.1.1.1.1 comment_id comment_parent_id  \\\n",
       "0                   0    hor72d8         t3_rhlfnx   \n",
       "2                   2    horc0ki        t1_hor72d8   \n",
       "3                   3    horebg5        t1_horc0ki   \n",
       "5                   5    hp8y40y        t1_hp8mmwv   \n",
       "8                   8    hosnjyl         t3_rhn77f   \n",
       "\n",
       "                                        comment_body comment_link_id  \\\n",
       "0  Deleting all comments and accounts in 3... 2.....       t3_rhlfnx   \n",
       "2  You joke. But, just another social media site ...       t3_rhlfnx   \n",
       "3  I only joke for now. Once it really happens, I...       t3_rhlfnx   \n",
       "5                                       In what way?       t3_rhlfnx   \n",
       "8  It will not have ther momentum of Donald Trump...       t3_rhn77f   \n",
       "\n",
       "   comment_score comment_subreddit  \\\n",
       "0              4       stockmarket   \n",
       "2              2       stockmarket   \n",
       "3              2       stockmarket   \n",
       "5              1       stockmarket   \n",
       "8             -2       stockmarket   \n",
       "\n",
       "                                          fixed_body  sentiment  \\\n",
       "0  Deleting all comments and accounts in 3... 2.....          1   \n",
       "2  You joke. But, just another social media site ...          2   \n",
       "3  I only joke for now. Once it really happens, I...          1   \n",
       "5                                       In what way?          1   \n",
       "8  It will not have ther momentum of Donald Trump...          2   \n",
       "\n",
       "   sentiment_adjusted  subcomment_count  subcomment_weight  upvote_weight  \\\n",
       "0                  -2               1.0           0.023256       0.007409   \n",
       "2                  -1               1.0           0.023256       0.007174   \n",
       "3                  -2               1.0           0.023256       0.007174   \n",
       "5                  -2               0.0           0.000000       0.007056   \n",
       "8                  -1               0.0           0.000000       0.006704   \n",
       "\n",
       "   weighted_sentiment_score  \n",
       "0                 -2.061675  \n",
       "2                 -1.030597  \n",
       "3                 -2.061193  \n",
       "5                 -2.014113  \n",
       "8                 -1.006704  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152802b-02b7-4433-9aaa-14f139d9bd8c",
   "metadata": {},
   "source": [
    "# Trim dataframe to list & dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f371e7b3-de15-47e6-9bfa-40235df1b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_trimm = posts[['title', 'body']]\n",
    "comments_trimm = comments[['comment_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8733ffb2-6e9e-47a8-b5b6-e6d4afaba857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(12/27) Monday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td># Good Monday morning traders and investors of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(12/15) Wednesday's Pre-Market Stock Movers &amp; ...</td>\n",
       "      <td>#Good morning traders and investors of the r/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(12/13) Monday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td># Good Monday morning traders and investors of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reddit Files Confidentially for IPO. As of Aug...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(12/14) Tuesday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td>#Good morning traders and investors of the r/S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    (12/27) Monday's Pre-Market Stock Movers & News   \n",
       "1  (12/15) Wednesday's Pre-Market Stock Movers & ...   \n",
       "2    (12/13) Monday's Pre-Market Stock Movers & News   \n",
       "3  Reddit Files Confidentially for IPO. As of Aug...   \n",
       "4   (12/14) Tuesday's Pre-Market Stock Movers & News   \n",
       "\n",
       "                                                body  \n",
       "0  # Good Monday morning traders and investors of...  \n",
       "1  #Good morning traders and investors of the r/S...  \n",
       "2  # Good Monday morning traders and investors of...  \n",
       "3                                                NaN  \n",
       "4  #Good morning traders and investors of the r/S...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_trimm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e5d41e-1f0c-4a79-baf8-a2d4e88953ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deleting all comments and accounts in 3... 2.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You joke. But, just another social media site ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I only joke for now. Once it really happens, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In what way?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It will not have ther momentum of Donald Trump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_body\n",
       "0  Deleting all comments and accounts in 3... 2.....\n",
       "2  You joke. But, just another social media site ...\n",
       "3  I only joke for now. Once it really happens, I...\n",
       "5                                       In what way?\n",
       "8  It will not have ther momentum of Donald Trump..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_trimm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db800718-bfc7-48fe-8a99-f9ccbb1bfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts\n",
    "posts_trimm = posts_trimm.to_string()\n",
    "comments_trimm = comments_trimm.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c26052c-a7b2-4964-9eae-293ff36666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonkenizer. Lemmatizer is used here to achieve full morphological analysis and accurately identify the lemma for each word, this is better than simply stemming. \n",
    "WNL = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d7275-b490-4652-a524-8b997f03b40b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc694c45-a1ed-47b9-aebb-fa557e657860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up texts by transforming to lower case, also removing apostrophe\n",
    "def lowerRep(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text\n",
    "\n",
    "# removing number\n",
    "def removeNum(text):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    return text\n",
    "\n",
    "# remove extra chars and stop words\n",
    "def removeChar(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text1 = nltk.Text(tokens)\n",
    "    text_content = [''.join(re.split(\"[ .,;:!?‘’``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text1]\n",
    "    return text_content\n",
    "\n",
    "# remove URLs and images\n",
    "def removeURL(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# remove ![img] string\n",
    "def removeImg(text):\n",
    "    text = re.sub(r'!\\[img]\\S+', '', text)\n",
    "    text = re.sub(r'emote', '', text)\n",
    "    return text\n",
    "\n",
    "# initial removal of stopwords and empty spaces\n",
    "def removeSpace(text, stopwords):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    text = [s for s in text if len(s) != 0]\n",
    "    return text\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(text):\n",
    "    text = [WNL.lemmatize(t) for t in text]\n",
    "    return text\n",
    "\n",
    "# tokenize\n",
    "def tokenize(text):\n",
    "    tokens = [nltk.word_tokenize(i) for i in text]\n",
    "    return tokens\n",
    "\n",
    "# bigrams dictionary\n",
    "def bigramDict(text):\n",
    "    bigrams_list = list(nltk.bigrams(text))\n",
    "    dictionary = [' '.join(tup) for tup in bigrams_list]\n",
    "    return dictionary\n",
    "\n",
    "# generate words frequency\n",
    "def wordsFreq(text):\n",
    "    vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "    bag_of_words = vectorizer.fit_transform(text)\n",
    "    vectorizer.vocabulary_\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c2442-d0eb-498e-8eed-d8ed7d20c750",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564b22bc-87b7-4032-8bbf-fcbadd6f6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "posts_stopwords = list(STOPWORDS) + [\"https\", \"png\", \"imgur\", \"n\", \"click\", \"reddit\", \"ashx\", \"will\", \"CHART\", \n",
    "                                     \"t\", \"ta\", \"st_c\", \"sma\", \"smsch_200p\", \"bb_20_2\", \"webp\", \"s\", \"l\", \n",
    "                                    \"stofu_b_14_3_3\", \"macd_b_12_26_9\", \"rsi_b_14\", \"sch_200p\", \"p\", \"d\", \"c\"]\n",
    "comments_stopwords = list(STOPWORDS) + [\"n\", \"nbsp\", \"http\", \"u\", \"s\", \"reddit\", \"will\", \"tth\", \"emote\", \n",
    "                                       \"gon\", \"na\", \"wan\", 'na', \"emote freeemotespack\", \"emote\", \"freeemotespack\", \n",
    "                                        \"and or\", \"or\", \"total\", \"submission\", \"comment\", \"buy\", \"got\", \"ta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f388a2-87c8-4db6-b5c9-c98a44faf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# string1 = \"![img](emote|t5_2th52|4258)\"\n",
    "# string2 = \"https://github.com/yyd859/RedditIPO-SentimentAnalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa3af9ef-5050-4ad8-a8cc-69e7ca3432ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# removeImg(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ed97da3-d10e-4e4a-ad2c-f6662d86cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# removeURL(string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62a37d-6898-4331-aead-6260580d444a",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72338282-e765-4f81-9c95-b46e57d8a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for posts\n",
    "posts_trimm = removeURL(posts_trimm)\n",
    "posts_trimm = removeImg(posts_trimm)\n",
    "posts_trimm = lowerRep(posts_trimm)\n",
    "posts_trimm = removeNum(posts_trimm)\n",
    "posts_content = removeChar(posts_trimm)\n",
    "posts_content = removeSpace(posts_content, posts_stopwords)\n",
    "posts_content = lemmatize(posts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5ec200-ebc9-48c1-a49f-55d83f6fb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for comments: this should take longer\n",
    "comments_trimm = removeURL(comments_trimm)\n",
    "comments_trimm = removeImg(comments_trimm)\n",
    "comments_trimm = lowerRep(comments_trimm)\n",
    "comments_trimm = removeNum(comments_trimm)\n",
    "comments_content = removeChar(comments_trimm)\n",
    "comments_content = removeSpace(comments_content, comments_stopwords)\n",
    "comments_content = lemmatize(comments_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343c74f-607d-4980-b5dd-f42a6e7f24ba",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15c25bb2-a816-41d6-b76c-87348b04e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate bigram dictionary \n",
    "posts_token = tokenize(posts_content)\n",
    "posts_bigram = bigramDict(posts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20f260b-1b23-4f38-956a-7d0b2458fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate bigram for comments\n",
    "comments_token = tokenize(comments_content)\n",
    "comments_bigram = bigramDict(comments_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9714f92e-24fe-4d57-bf43-02432ae82558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# print(posts_token)\n",
    "# print(posts_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0f0393-5fa7-485f-b967-1658edb8a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for each bigram for posts\n",
    "posts_word_freq = wordsFreq(posts_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41bfb68-e406-4838-a446-46bb659fd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# print(posts_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16b133e8-247c-4608-9053-af8cf36c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for each bigram for comments\n",
    "comments_word_freq = wordsFreq(comments_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6658bef-5a82-42a6-856c-f8e6e4e35e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eeaa137-c913-4dd3-88fa-00d4e3972bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_dict = dict(posts_word_freq)\n",
    "comments_dict = dict(comments_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74313112-508d-49a2-a2cb-98317c7e3382",
   "metadata": {},
   "source": [
    "# Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca914e16-3e74-48b7-a5fb-a5eb2f881785",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_posts = WordCloud(\n",
    "        background_color = 'white',\n",
    "        stopwords = posts_stopwords,\n",
    "        height = 400,\n",
    "        width = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "347d34da-e0a4-4fa7-bd75-0aa5524010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_comments = WordCloud(\n",
    "        background_color = 'white', \n",
    "        stopwords = comments_stopwords, \n",
    "        height = 400, \n",
    "        width = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18be5dbe-ca56-45eb-a28d-1a6c1a48a44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x195c9221b50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.generate_from_frequencies(posts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d86d94b7-ccd4-4629-a6fc-7524abc25af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x195c98a56d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.generate_from_frequencies(comments_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac000ec1-d3c5-42bc-b715-1a8d09e3ad3a",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebb1ba1a-38f7-4412-aa8d-af786e23c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_colors = wc_posts.ImageColorGenerator(mask)\n",
    "# wc_posts.recolor(color_func=image_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b9477be-c65c-45c6-b287-f1f2c420be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(wc_posts, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "526dac70-e3a2-4aa9-9633-907c6dcebf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = \"C:/Users/kevin/OneDrive/Winter Case Competition/RedditIPO-SentimentAnalysis/picture\"\n",
    "os.chdir(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f2d3ed4-0a6b-4265-a29a-eb729dc897b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x195c9221b50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.to_file('wc_post_bigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "058ab404-220c-41dc-8c3c-ccf966cb0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x195c98a56d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.to_file('wc_comment_bigram.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

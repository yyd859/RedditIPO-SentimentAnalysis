{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ee84b6-6413-4330-90ad-fd00d889dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\kevin\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\kevin\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: regex in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee4c2f-f679-4ec4-9637-03b5927a2c91",
   "metadata": {},
   "source": [
    "# Packages and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6688c4-ad5b-4b10-8b6c-3c3b75ccd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500dd145-1d88-4f12-bc84-56713f69df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9d96d6-b227-4fea-a080-8f7d69836f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"C:/Users/kevin/OneDrive/Winter Case Competition/RedditIPO-SentimentAnalysis/Data\"\n",
    "os.chdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e06b1a6-fa33-411a-b12f-cd19653d82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading text\n",
    "posts = pd.read_csv('posts_rd.csv')\n",
    "comments = pd.read_csv('comments_reddit_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdeac124-ee5d-49b8-ab19-cfa766feb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.array(Image.open(\"reddit_icon.png\"))\n",
    "# comments=comments[comments.sentiment_adjusted<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d98bb9-3e1b-42f4-91eb-95bfde3bd69f",
   "metadata": {},
   "source": [
    "## A few check points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1592e4a-f31a-4f5c-b6ef-929fcc109cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Redditors are pledging to avoid Robinhood's IP...</td>\n",
       "      <td>2289</td>\n",
       "      <td>od9iun</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>279</td>\n",
       "      <td>1.625358e+09</td>\n",
       "      <td>[Redditors are pledging to avoid Robinhood's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(2/8) Monday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td>411</td>\n",
       "      <td>lfc2xk</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>78</td>\n",
       "      <td>1.612792e+09</td>\n",
       "      <td>#Good morning traders and investors of the r/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I conducted research on the growth of the audi...</td>\n",
       "      <td>170</td>\n",
       "      <td>mhicut</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>13</td>\n",
       "      <td>1.617233e+09</td>\n",
       "      <td>Iâ€™ve been collecting data from major social ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>PLTR Financial Due Diligence Update Q2-21</td>\n",
       "      <td>134</td>\n",
       "      <td>pg0osz</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>22</td>\n",
       "      <td>1.630523e+09</td>\n",
       "      <td>Disclaimer: I'm not a financial advisor nor is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Nerds On Site - Q2 results are out + potential...</td>\n",
       "      <td>94</td>\n",
       "      <td>lscd9x</td>\n",
       "      <td>https://www.reddit.com/r/StockMarket/comments/...</td>\n",
       "      <td>27</td>\n",
       "      <td>1.614275e+09</td>\n",
       "      <td>Hi all, Nerds On Site released their Q2 result...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  \\\n",
       "0           0             0               0   \n",
       "1           1             1               1   \n",
       "2           2             2               2   \n",
       "3           3             3               3   \n",
       "4           4             4               4   \n",
       "\n",
       "                                               title  score      id  \\\n",
       "0  Redditors are pledging to avoid Robinhood's IP...   2289  od9iun   \n",
       "1      (2/8) Monday's Pre-Market Stock Movers & News    411  lfc2xk   \n",
       "2  I conducted research on the growth of the audi...    170  mhicut   \n",
       "3          PLTR Financial Due Diligence Update Q2-21    134  pg0osz   \n",
       "4  Nerds On Site - Q2 results are out + potential...     94  lscd9x   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0  https://www.reddit.com/r/StockMarket/comments/...        279  1.625358e+09   \n",
       "1  https://www.reddit.com/r/StockMarket/comments/...         78  1.612792e+09   \n",
       "2  https://www.reddit.com/r/StockMarket/comments/...         13  1.617233e+09   \n",
       "3  https://www.reddit.com/r/StockMarket/comments/...         22  1.630523e+09   \n",
       "4  https://www.reddit.com/r/StockMarket/comments/...         27  1.614275e+09   \n",
       "\n",
       "                                                body  \n",
       "0   [Redditors are pledging to avoid Robinhood's ...  \n",
       "1  #Good morning traders and investors of the r/S...  \n",
       "2  Iâ€™ve been collecting data from major social ne...  \n",
       "3  Disclaimer: I'm not a financial advisor nor is...  \n",
       "4  Hi all, Nerds On Site released their Q2 result...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e30a86-fbd8-4f4e-9cf9-ee6ca002a6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_parent_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_link_id</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>comment_subreddit</th>\n",
       "      <th>comment_time_stamp</th>\n",
       "      <th>subcomment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hc1lokp</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>I would so buy Reddit stock!</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>7</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>2021-09-08 06:55:21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hc265jx</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>I would buy</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>5</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>2021-09-08 10:05:58</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hc1o9v0</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>Sure thing the next Reddit will be pleased. It...</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>2</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>2021-09-08 07:26:03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>hc1spix</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>Depends on the price compared to all their num...</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>1</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>2021-09-08 08:12:55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>hc1tg4v</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>I'd buy that for a dollar or $20 ðŸš€ðŸš€ðŸš€ðŸš€ðŸ¤£ðŸ¤£ðŸ¤£</td>\n",
       "      <td>t3_pk815c</td>\n",
       "      <td>1</td>\n",
       "      <td>stockmarket</td>\n",
       "      <td>2021-09-08 08:20:09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0           0             0               0                 0   \n",
       "1           1             1               1                 1   \n",
       "2           2             2               2                 2   \n",
       "3           3             3               3                 3   \n",
       "4           4             4               4                 4   \n",
       "\n",
       "   Unnamed: 0.1.1.1.1 comment_id comment_parent_id  \\\n",
       "0                   0    hc1lokp         t3_pk815c   \n",
       "1                   1    hc265jx         t3_pk815c   \n",
       "2                   2    hc1o9v0         t3_pk815c   \n",
       "3                   3    hc1spix         t3_pk815c   \n",
       "4                   4    hc1tg4v         t3_pk815c   \n",
       "\n",
       "                                        comment_body comment_link_id  \\\n",
       "0                       I would so buy Reddit stock!       t3_pk815c   \n",
       "1                                        I would buy       t3_pk815c   \n",
       "2  Sure thing the next Reddit will be pleased. It...       t3_pk815c   \n",
       "3  Depends on the price compared to all their num...       t3_pk815c   \n",
       "4           I'd buy that for a dollar or $20 ðŸš€ðŸš€ðŸš€ðŸš€ðŸ¤£ðŸ¤£ðŸ¤£       t3_pk815c   \n",
       "\n",
       "   comment_score comment_subreddit   comment_time_stamp  subcomment_count  \n",
       "0              7       stockmarket  2021-09-08 06:55:21               NaN  \n",
       "1              5       stockmarket  2021-09-08 10:05:58               NaN  \n",
       "2              2       stockmarket  2021-09-08 07:26:03               NaN  \n",
       "3              1       stockmarket  2021-09-08 08:12:55               NaN  \n",
       "4              1       stockmarket  2021-09-08 08:20:09               NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152802b-02b7-4433-9aaa-14f139d9bd8c",
   "metadata": {},
   "source": [
    "# Trim dataframe to list & dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f371e7b3-de15-47e6-9bfa-40235df1b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_trimm = posts[['title', 'body']]\n",
    "comments_trimm = comments[['comment_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8733ffb2-6e9e-47a8-b5b6-e6d4afaba857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Redditors are pledging to avoid Robinhood's IP...</td>\n",
       "      <td>[Redditors are pledging to avoid Robinhood's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2/8) Monday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td>#Good morning traders and investors of the r/S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I conducted research on the growth of the audi...</td>\n",
       "      <td>Iâ€™ve been collecting data from major social ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PLTR Financial Due Diligence Update Q2-21</td>\n",
       "      <td>Disclaimer: I'm not a financial advisor nor is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nerds On Site - Q2 results are out + potential...</td>\n",
       "      <td>Hi all, Nerds On Site released their Q2 result...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Redditors are pledging to avoid Robinhood's IP...   \n",
       "1      (2/8) Monday's Pre-Market Stock Movers & News   \n",
       "2  I conducted research on the growth of the audi...   \n",
       "3          PLTR Financial Due Diligence Update Q2-21   \n",
       "4  Nerds On Site - Q2 results are out + potential...   \n",
       "\n",
       "                                                body  \n",
       "0   [Redditors are pledging to avoid Robinhood's ...  \n",
       "1  #Good morning traders and investors of the r/S...  \n",
       "2  Iâ€™ve been collecting data from major social ne...  \n",
       "3  Disclaimer: I'm not a financial advisor nor is...  \n",
       "4  Hi all, Nerds On Site released their Q2 result...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_trimm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e5d41e-1f0c-4a79-baf8-a2d4e88953ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would so buy Reddit stock!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I would buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sure thing the next Reddit will be pleased. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Depends on the price compared to all their num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'd buy that for a dollar or $20 ðŸš€ðŸš€ðŸš€ðŸš€ðŸ¤£ðŸ¤£ðŸ¤£</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_body\n",
       "0                       I would so buy Reddit stock!\n",
       "1                                        I would buy\n",
       "2  Sure thing the next Reddit will be pleased. It...\n",
       "3  Depends on the price compared to all their num...\n",
       "4           I'd buy that for a dollar or $20 ðŸš€ðŸš€ðŸš€ðŸš€ðŸ¤£ðŸ¤£ðŸ¤£"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_trimm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db800718-bfc7-48fe-8a99-f9ccbb1bfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts\n",
    "posts_trimm = posts_trimm.to_string()\n",
    "comments_trimm = comments_trimm.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c26052c-a7b2-4964-9eae-293ff36666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tonkenizer. Lemmatizer is used here to achieve full morphological analysis and accurately identify the lemma for each word, this is better than simply stemming. \n",
    "WNL = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d7275-b490-4652-a524-8b997f03b40b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc694c45-a1ed-47b9-aebb-fa557e657860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up texts by transforming to lower case, also removing apostrophe\n",
    "def lowerRep(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text\n",
    "\n",
    "# removing number\n",
    "def removeNum(text):\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    return text\n",
    "\n",
    "# remove extra chars and stop words\n",
    "def removeChar(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text1 = nltk.Text(tokens)\n",
    "    text_content = [''.join(re.split(\"[ .,;:!?â€˜â€™``''@#$%^_&*()<>{}~\\n\\t\\\\\\-]\", word)) for word in text1]\n",
    "    return text_content\n",
    "\n",
    "# remove URLs and images\n",
    "def removeURL(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# remove ![img] string\n",
    "def removeImg(text):\n",
    "    text = re.sub(r'!\\[img]\\S+', '', text)\n",
    "    text = re.sub(r'emote', '', text)\n",
    "    return text\n",
    "\n",
    "# initial removal of stopwords and empty spaces\n",
    "def removeSpace(text, stopwords):\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    text = [s for s in text if len(s) != 0]\n",
    "    return text\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(text):\n",
    "    text = [WNL.lemmatize(t) for t in text]\n",
    "    return text\n",
    "\n",
    "# tokenize\n",
    "def tokenize(text):\n",
    "    tokens = [nltk.word_tokenize(i) for i in text]\n",
    "    return tokens\n",
    "\n",
    "# bigrams dictionary\n",
    "def bigramDict(text):\n",
    "    bigrams_list = list(nltk.bigrams(text))\n",
    "    dictionary = [' '.join(tup) for tup in bigrams_list]\n",
    "    return dictionary\n",
    "\n",
    "# generate words frequency\n",
    "def wordsFreq(text):\n",
    "    vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "    bag_of_words = vectorizer.fit_transform(text)\n",
    "    vectorizer.vocabulary_\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c2442-d0eb-498e-8eed-d8ed7d20c750",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564b22bc-87b7-4032-8bbf-fcbadd6f6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "posts_stopwords = list(STOPWORDS) + [\"https\", \"png\", \"imgur\", \"n\", \"click\", \"reddit\", \"ashx\", \"will\", \"CHART\", \n",
    "                                     \"t\", \"ta\", \"st_c\", \"sma\", \"smsch_200p\", \"bb_20_2\", \"webp\", \"s\", \"l\", \n",
    "                                    \"stofu_b_14_3_3\", \"macd_b_12_26_9\", \"rsi_b_14\", \"sch_200p\", \"p\", \"d\", \"c\", \n",
    "                                    \"nn\", \"xb\", \"live\"]\n",
    "comments_stopwords = list(STOPWORDS) + [\"n\", \"nbsp\", \"http\", \"u\", \"s\", \"reddit\", \"will\", \"tth\", \"emote\", \n",
    "                                       \"gon\", \"na\", \"wan\", 'na', \"emote freeemotespack\", \"emote\", \"freeemotespack\", \n",
    "                                        \"and or\", \"or\", \"total\", \"submission\", \"comment\", \"buy\", \"got\", \"ta\", \"message compose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f388a2-87c8-4db6-b5c9-c98a44faf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# string1 = \"![img](emote|t5_2th52|4258)\"\n",
    "# string2 = \"https://github.com/yyd859/RedditIPO-SentimentAnalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa3af9ef-5050-4ad8-a8cc-69e7ca3432ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# removeImg(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ed97da3-d10e-4e4a-ad2c-f6662d86cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# removeURL(string2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e62a37d-6898-4331-aead-6260580d444a",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72338282-e765-4f81-9c95-b46e57d8a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for posts\n",
    "posts_trimm = removeURL(posts_trimm)\n",
    "posts_trimm = removeImg(posts_trimm)\n",
    "posts_trimm = lowerRep(posts_trimm)\n",
    "posts_trimm = removeNum(posts_trimm)\n",
    "posts_content = removeChar(posts_trimm)\n",
    "posts_content = removeSpace(posts_content, posts_stopwords)\n",
    "posts_content = lemmatize(posts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5ec200-ebc9-48c1-a49f-55d83f6fb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing for comments: this should take longer\n",
    "comments_trimm = removeURL(comments_trimm)\n",
    "comments_trimm = removeImg(comments_trimm)\n",
    "comments_trimm = lowerRep(comments_trimm)\n",
    "comments_trimm = removeNum(comments_trimm)\n",
    "comments_content = removeChar(comments_trimm)\n",
    "comments_content = removeSpace(comments_content, comments_stopwords)\n",
    "comments_content = lemmatize(comments_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343c74f-607d-4980-b5dd-f42a6e7f24ba",
   "metadata": {},
   "source": [
    "## Tokenization & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15c25bb2-a816-41d6-b76c-87348b04e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate bigram dictionary \n",
    "posts_token = tokenize(posts_content)\n",
    "posts_bigram = bigramDict(posts_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20f260b-1b23-4f38-956a-7d0b2458fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and generate bigram for comments\n",
    "comments_token = tokenize(comments_content)\n",
    "comments_bigram = bigramDict(comments_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9714f92e-24fe-4d57-bf43-02432ae82558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# print(posts_token)\n",
    "# print(posts_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0f0393-5fa7-485f-b967-1658edb8a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for each bigram for posts\n",
    "posts_word_freq = wordsFreq(posts_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41bfb68-e406-4838-a446-46bb659fd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# print(posts_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16b133e8-247c-4608-9053-af8cf36c94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frequency for each bigram for comments\n",
    "comments_word_freq = wordsFreq(comments_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6658bef-5a82-42a6-856c-f8e6e4e35e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7332f7-2244-4739-a1b0-c1c4c80051af",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03046304-90a8-4a9c-afe9-8c06aba0d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_frequency = pd.DataFrame(comments_word_freq)\n",
    "post_frequency = pd.DataFrame(posts_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f598811-4b7a-46aa-b416-49bf442db03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_frequency.to_csv(\"comment_bigram_frequency.csv\")\n",
    "post_frequency.to_csv(\"post_bigram_frequency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8eeaa137-c913-4dd3-88fa-00d4e3972bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_dict = dict(posts_word_freq)\n",
    "comments_dict = dict(comments_word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74313112-508d-49a2-a2cb-98317c7e3382",
   "metadata": {},
   "source": [
    "# Word Cloud Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca914e16-3e74-48b7-a5fb-a5eb2f881785",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_posts = WordCloud(\n",
    "        background_color = 'white',\n",
    "        stopwords = posts_stopwords,\n",
    "        height = 400,\n",
    "        width = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "347d34da-e0a4-4fa7-bd75-0aa5524010e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_comments = WordCloud(\n",
    "        background_color = 'white', \n",
    "        stopwords = comments_stopwords, \n",
    "        height = 400, \n",
    "        width = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18be5dbe-ca56-45eb-a28d-1a6c1a48a44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1716b6d3190>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.generate_from_frequencies(posts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d86d94b7-ccd4-4629-a6fc-7524abc25af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1716b6d37c0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.generate_from_frequencies(comments_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac000ec1-d3c5-42bc-b715-1a8d09e3ad3a",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebb1ba1a-38f7-4412-aa8d-af786e23c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_colors = wc_posts.ImageColorGenerator(mask)\n",
    "# wc_posts.recolor(color_func=image_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b9477be-c65c-45c6-b287-f1f2c420be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(wc_posts, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "526dac70-e3a2-4aa9-9633-907c6dcebf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = \"C:/Users/kevin/OneDrive/Winter Case Competition/RedditIPO-SentimentAnalysis/picture/Word Clouds\"\n",
    "os.chdir(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f2d3ed4-0a6b-4265-a29a-eb729dc897b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1716b6d3190>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_posts.to_file('wc_post_bigram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "058ab404-220c-41dc-8c3c-ccf966cb0ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1716b6d37c0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc_comments.to_file('wc_comment_bigram.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
